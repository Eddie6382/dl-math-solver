{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJRXwCxGrmtN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f0b9787a-bc31-4e49-cf1c-7bfb8baf0ca5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Aug 14 05:17:56 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80vhbiPy-p7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "c36e2816-b351-4014-c1e0-cdd251054bee"
      },
      "source": [
        "!wget -O data.tgz https://gntuedutw-my.sharepoint.com/:u:/g/personal/b07901020_g_ntu_edu_tw/EY6n-UwrWhVLibJcYPUpn60BvHPqkurEo2CqA1uDaBpYpg?download=1\n",
        "!tar zxvf data.tgz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-14 05:17:58--  https://gntuedutw-my.sharepoint.com/:u:/g/personal/b07901020_g_ntu_edu_tw/EY6n-UwrWhVLibJcYPUpn60BvHPqkurEo2CqA1uDaBpYpg?download=1\n",
            "Resolving gntuedutw-my.sharepoint.com (gntuedutw-my.sharepoint.com)... 40.108.228.30\n",
            "Connecting to gntuedutw-my.sharepoint.com (gntuedutw-my.sharepoint.com)|40.108.228.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/b07901020_g_ntu_edu_tw/Documents/Lecture%20Notes,%202019%20Summer%20Interns/info_data/data.tgz?&originalPath=aHR0cHM6Ly9nbnR1ZWR1dHctbXkuc2hhcmVwb2ludC5jb20vOnU6L2cvcGVyc29uYWwvYjA3OTAxMDIwX2dfbnR1X2VkdV90dy9FWTZuLVV3cldoVkxpYkpjWVBVcG42MEJ2SFBxa3VyRW8yQ3FBMXVEYUJwWXBnP3J0aW1lPU9GZ2VhUkZBMkVn [following]\n",
            "--2020-08-14 05:17:59--  https://gntuedutw-my.sharepoint.com/personal/b07901020_g_ntu_edu_tw/Documents/Lecture%20Notes,%202019%20Summer%20Interns/info_data/data.tgz?&originalPath=aHR0cHM6Ly9nbnR1ZWR1dHctbXkuc2hhcmVwb2ludC5jb20vOnU6L2cvcGVyc29uYWwvYjA3OTAxMDIwX2dfbnR1X2VkdV90dy9FWTZuLVV3cldoVkxpYkpjWVBVcG42MEJ2SFBxa3VyRW8yQ3FBMXVEYUJwWXBnP3J0aW1lPU9GZ2VhUkZBMkVn\n",
            "Reusing existing connection to gntuedutw-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1743906 (1.7M) [application/x-compressed]\n",
            "Saving to: ‘data.tgz’\n",
            "\n",
            "data.tgz            100%[===================>]   1.66M  1.33MB/s    in 1.2s    \n",
            "\n",
            "2020-08-14 05:18:02 (1.33 MB/s) - ‘data.tgz’ saved [1743906/1743906]\n",
            "\n",
            "._data\n",
            "data/\n",
            "data/.DS_Store\n",
            "data/._dataSetMul1-1.docx\n",
            "data/dataSetMul1-1.docx\n",
            "data/._dataSetSub2-1.docx\n",
            "data/dataSetSub2-1.docx\n",
            "data/._dataSetDiv2-1.docx\n",
            "data/dataSetDiv2-1.docx\n",
            "data/._dataSetAdd2-1.docx\n",
            "data/dataSetAdd2-1.docx\n",
            "data/._dataSetMul3-1.docx\n",
            "data/dataSetMul3-1.docx\n",
            "data/._dataSetSub3-1.docx\n",
            "data/dataSetSub3-1.docx\n",
            "data/._dataSetAdd1-1.docx\n",
            "data/dataSetAdd1-1.docx\n",
            "data/._dataSetDiv1-1.docx\n",
            "data/dataSetDiv1-1.docx\n",
            "data/._dataSetMul2-1.docx\n",
            "data/dataSetMul2-1.docx\n",
            "data/._OneVariableSingleEQ.json\n",
            "data/OneVariableSingleEQ.json\n",
            "data/._dataSetSub1-1.docx\n",
            "data/dataSetSub1-1.docx\n",
            "data/._dataSetAdd3-1.docx\n",
            "data/dataSetAdd3-1.docx\n",
            "data/._dataSetDiv3-1.docx\n",
            "data/dataSetDiv3-1.docx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1wmVv5r-rs3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "5165883a-bbf3-4f6f-ade0-60e7596aacee"
      },
      "source": [
        "!wget -O data_final.pkl https://gntuedutw-my.sharepoint.com/:u:/g/personal/b07901020_g_ntu_edu_tw/EUa3CoZ5Bq5Ivam1RWLXNPgBLzQC5o-c9Ww-avxFW0SEQw?download=1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-14 05:18:06--  https://gntuedutw-my.sharepoint.com/:u:/g/personal/b07901020_g_ntu_edu_tw/EUa3CoZ5Bq5Ivam1RWLXNPgBLzQC5o-c9Ww-avxFW0SEQw?download=1\n",
            "Resolving gntuedutw-my.sharepoint.com (gntuedutw-my.sharepoint.com)... 40.108.228.30\n",
            "Connecting to gntuedutw-my.sharepoint.com (gntuedutw-my.sharepoint.com)|40.108.228.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/b07901020_g_ntu_edu_tw/Documents/Lecture%20Notes,%202019%20Summer%20Interns/info_data/data_final.pkl?&originalPath=aHR0cHM6Ly9nbnR1ZWR1dHctbXkuc2hhcmVwb2ludC5jb20vOnU6L2cvcGVyc29uYWwvYjA3OTAxMDIwX2dfbnR1X2VkdV90dy9FVWEzQ29aNUJxNUl2YW0xUldMWE5QZ0JMelFDNW8tYzlXdy1hdnhGVzBTRVF3P3J0aW1lPWx4S2RiUkZBMkVn [following]\n",
            "--2020-08-14 05:18:07--  https://gntuedutw-my.sharepoint.com/personal/b07901020_g_ntu_edu_tw/Documents/Lecture%20Notes,%202019%20Summer%20Interns/info_data/data_final.pkl?&originalPath=aHR0cHM6Ly9nbnR1ZWR1dHctbXkuc2hhcmVwb2ludC5jb20vOnU6L2cvcGVyc29uYWwvYjA3OTAxMDIwX2dfbnR1X2VkdV90dy9FVWEzQ29aNUJxNUl2YW0xUldMWE5QZ0JMelFDNW8tYzlXdy1hdnhGVzBTRVF3P3J0aW1lPWx4S2RiUkZBMkVn\n",
            "Reusing existing connection to gntuedutw-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5716458 (5.5M) [application/octet-stream]\n",
            "Saving to: ‘data_final.pkl’\n",
            "\n",
            "data_final.pkl      100%[===================>]   5.45M  2.61MB/s    in 2.1s    \n",
            "\n",
            "2020-08-14 05:18:09 (2.61 MB/s) - ‘data_final.pkl’ saved [5716458/5716458]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc3TfwQ7PZhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ee0d514e-dfef-4803-c275-0dc9c623cb85"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x # enable TF 2.x in Colab\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "smoothie = SmoothingFunction()\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x # enable TF 2.x in Colab`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnCgKUrrPjQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "23581d1a-c623-4b5b-eb88-a32cf4e0e698"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u2YzspHkstP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_root = 'data'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wNVhL5BxZZsh"
      },
      "source": [
        "### Creating the dataset of word problems\n",
        "\n",
        "*Please add the correct path to load the data file*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZhTdlS9DZY3W",
        "colab": {}
      },
      "source": [
        "# laoding data (.pkl file)\n",
        "with open('data_final.pkl', 'rb') as f:\n",
        "  df = pickle.load(f)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP7BXTKZ-9ZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "933949ad-33e9-482e-f91c-593ccbaa48a6"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38144, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9aGO66p-_b8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1f45cbf7-e1af-4c00-9d43-bed16228b222"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Equation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>A painter needed to paint 12 rooms in a build...</td>\n",
              "      <td>X=(7.0*(12.0-5.0))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3897</th>\n",
              "      <td>Brenda had 253 raspberry. John gripped some ra...</td>\n",
              "      <td>X = 253 - 66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27626</th>\n",
              "      <td>Casey wants to share some Bread among 17 frien...</td>\n",
              "      <td>X = 39 * 17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32530</th>\n",
              "      <td>Liza had 34 Press. Thomas furnished him some m...</td>\n",
              "      <td>X = 64 - 34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16266</th>\n",
              "      <td>George wants to distribute 125 mangos among 25...</td>\n",
              "      <td>X = 125 / 25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Question            Equation\n",
              "963     A painter needed to paint 12 rooms in a build...  X=(7.0*(12.0-5.0))\n",
              "3897   Brenda had 253 raspberry. John gripped some ra...        X = 253 - 66\n",
              "27626  Casey wants to share some Bread among 17 frien...         X = 39 * 17\n",
              "32530  Liza had 34 Press. Thomas furnished him some m...         X = 64 - 34\n",
              "16266  George wants to distribute 125 mangos among 25...        X = 125 / 25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52CngavAg1PM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_exps = list(df['Question'].values)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JFUvCPCP-jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_eqn(eqn):\n",
        "  '''\n",
        "  Add a space between every character in the equation string.\n",
        "  Eg: 'x = 23 + 88' becomes 'x =  2 3 + 8 8'\n",
        "  '''\n",
        "  elements = list(eqn)\n",
        "  return ' '.join(elements)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BP-L86thNKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_exps = list(df['Equation'].apply(lambda x: convert_eqn(x)).values)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTRQklLBp-YC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "010a2cef-599d-4be4-bf5a-bb04d395e574"
      },
      "source": [
        "# Input: Word problem\n",
        "input_exps[:5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' A painter needed to paint 12 rooms in a building. Each room takes 7 hours to paint. If he already painted 5 rooms, how much longer will he take to paint the rest? ',\n",
              " 'Brenda had 253 raspberry. John gripped some raspberry. Now Brenda has 66  raspberry. How many did John grippeds?',\n",
              " 'Casey wants to share some Bread among 17 friends.If each friend get 39 Bread, then how many Bread john would have?',\n",
              " 'Liza had 34 Press. Thomas furnished him some more. Now Liza has 64 Press. How many did Thomas furnish him?',\n",
              " 'George wants to distribute 125 mangos among 25 friends. How many would each friend acquire?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9DL8tD-6iP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9a928671-a4e6-4765-9428-29569544d5f1"
      },
      "source": [
        "# Target: Equation\n",
        "target_exps[:5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['X = ( 7 . 0 * ( 1 2 . 0 - 5 . 0 ) )',\n",
              " 'X   =   2 5 3   -   6 6',\n",
              " 'X   =   3 9   *   1 7',\n",
              " 'X   =   6 4   -   3 4',\n",
              " 'X   =   1 2 5   /   2 5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_MAlVH39Mlh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6505c7ee-f251-4a62-be07-1508c0915ea8"
      },
      "source": [
        "len(pd.Series(input_exps)), len(pd.Series(input_exps).unique())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38144, 38144)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL3-dcn8YMe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d3052b4-265e-4e78-b3e7-71973ea5fe05"
      },
      "source": [
        "len(pd.Series(target_exps)), len(pd.Series(target_exps).unique())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38144, 23603)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-0vtG7rdZdy",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing and Tokenizing the Input and Target exps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yx7HUtFYZri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_input(sentence):\n",
        "  '''\n",
        "  For the word problem, convert everything to lowercase, add spaces around all\n",
        "  punctuations and digits, and remove any extra spaces. \n",
        "  '''\n",
        "  sentence = sentence.lower().strip()\n",
        "  sentence = re.sub(r\"([?.!,’])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r\"([0-9])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "  sentence = sentence.rstrip().strip()\n",
        "  return sentence"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfyWFYbo-Fkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_target(sentence):\n",
        "  '''\n",
        "  For the equation, convert it to lowercase and remove extra spaces\n",
        "  '''\n",
        "  sentence = sentence.lower().strip()\n",
        "  return sentence"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEAS9242ZUT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessed_input_exps = list(map(preprocess_input, input_exps))    # map function through 'map', 'preprocess_input is the name of function\n",
        "preprocessed_target_exps = list(map(preprocess_target, target_exps))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy-bGm81a8yl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0c35b06f-d787-4dfe-f52a-d5fe83fefd6f"
      },
      "source": [
        "preprocessed_input_exps[:5]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a painter needed to paint 1 2 rooms in a building . each room takes 7 hours to paint . if he already painted 5 rooms , how much longer will he take to paint the rest ?',\n",
              " 'brenda had 2 5 3 raspberry . john gripped some raspberry . now brenda has 6 6 raspberry . how many did john grippeds ?',\n",
              " 'casey wants to share some bread among 1 7 friends . if each friend get 3 9 bread , then how many bread john would have ?',\n",
              " 'liza had 3 4 press . thomas furnished him some more . now liza has 6 4 press . how many did thomas furnish him ?',\n",
              " 'george wants to distribute 1 2 5 mangos among 2 5 friends . how many would each friend acquire ?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g15oK28mbA89",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "38afa3c6-7b47-40eb-b4d0-4bc1ece3b5b5"
      },
      "source": [
        "preprocessed_target_exps[:5]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['x = ( 7 . 0 * ( 1 2 . 0 - 5 . 0 ) )',\n",
              " 'x   =   2 5 3   -   6 6',\n",
              " 'x   =   3 9   *   1 7',\n",
              " 'x   =   6 4   -   3 4',\n",
              " 'x   =   1 2 5   /   2 5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKhkEBsfbJaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  '''\n",
        "  Tokenize the given list of strings and return the tokenized output\n",
        "  along with the fitted tokenizer.\n",
        "  '''\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)            # This method creates the vocabulary index based on word frequency, like {'the':1, 'earth':2, .......}\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL_eLZZAbsHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(preprocessed_input_exps)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI2k73bDceJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f88990a-5919-40bc-dbc1-0a637da6c979"
      },
      "source": [
        "len(inp_lang_tokenizer.word_index)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5887"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVIuwYu9b7fH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_tensor, targ_lang_tokenizer = tokenize(preprocessed_target_exps)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqLOtR5h5s9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "old_len = len(targ_lang_tokenizer.word_index)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG9Nq0ZkBVZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp_len = len(inp_lang_tokenizer.word_index)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0drKV5Vx6k2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def append_start_end(x,last_int):\n",
        "  '''\n",
        "  Add integers for start and end tokens for input/target exps\n",
        "  '''\n",
        "  l = []\n",
        "  l.append(last_int+1)\n",
        "  l.extend(x)\n",
        "  l.append(last_int+2)\n",
        "  return l"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HepsumSnyLL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor_list = [append_start_end(i,len(inp_lang_tokenizer.word_index)) for i in input_tensor]\n",
        "target_tensor_list = [append_start_end(i,len(targ_lang_tokenizer.word_index)) for i in target_tensor]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onOq9Cj4CajL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad (at the end) all sequences such that they are of equal length\n",
        "input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor_list, padding='post')\n",
        "target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor_list, padding='post')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da9_X2TF0qe-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3f237afc-8eb3-4d07-99e4-68f56aabcd7b"
      },
      "source": [
        "input_tensor"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5888,  104, 2454, ...,    0,    0,    0],\n",
              "       [5888,  280,    6, ...,    0,    0,    0],\n",
              "       [5888,  811,   43, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [5888,  380,   12, ...,    0,    0,    0],\n",
              "       [5888,  167,    6, ...,    0,    0,    0],\n",
              "       [5888,  404,    6, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vimEgxw8tCjz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a3f0fe4a-d755-4939-c808-7a4db7ead548"
      },
      "source": [
        "target_tensor"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[46,  2,  1, ...,  0,  0,  0],\n",
              "       [46,  2,  1, ...,  0,  0,  0],\n",
              "       [46,  2,  1, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [46,  2,  1, ...,  0,  0,  0],\n",
              "       [46,  2,  1, ...,  0,  0,  0],\n",
              "       [46,  2,  1, ...,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bpeONmscRzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here we are increasing the vocabulary size of the target, by adding a\n",
        "# few extra vocabulary words (which will not actually be used) as otherwise the\n",
        "# small vocab size causes issues downstream in the network.\n",
        "keys = [str(i) for i in range(10,51)]\n",
        "for i,k in enumerate(keys):\n",
        "  targ_lang_tokenizer.word_index[k]=len(targ_lang_tokenizer.word_index)+i+4"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbO9GO0sb4-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6c14915-fe0e-4f6c-cfa4-e0c4e2b26d8c"
      },
      "source": [
        "len(targ_lang_tokenizer.word_index)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKtDz1pdV11",
        "colab_type": "text"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWn3alPwcIMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating training and validation sets\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n",
        "                                                                                                target_tensor,\n",
        "                                                                                                test_size=0.05,\n",
        "                                                                                                random_state=42)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkvMMb7NUG1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71d50f6d-51a4-487a-f6f4-54f436a04216"
      },
      "source": [
        "len(input_tensor_train)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8VnUqe9UE9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c68227ae-528f-4216-800d-47918b434423"
      },
      "source": [
        "len(input_tensor_val)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1908"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFxzn930dDNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "input_vocab_size = len(inp_lang_tokenizer.word_index)+3\n",
        "target_vocab_size = len(targ_lang_tokenizer.word_index)+3\n",
        "dropout_rate = 0.0"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BnLEanaeNOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9aa165d9-6801-4068-cebd-f93de8e3aec4"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 124]), TensorShape([64, 66]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLihV1nlcqH8",
        "colab_type": "text"
      },
      "source": [
        "### Transformer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRxTAfRkdRh5",
        "colab_type": "text"
      },
      "source": [
        "#### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqjLcutlcubG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We provide positional information about the data to the model,\n",
        "# otherwise each sentence will be treated as Bag of Words\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4LBjCfbd36x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0ARP8oYeGpb",
        "colab_type": "text"
      },
      "source": [
        "#### Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPU51VjMeIOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mask all elements are that not words (padding) so that it is not treated as input\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLLyLy3_eS69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsFCUWbNz-jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVbSOYuyeaPm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW-49jyBeZy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhCANAddefdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq-Kw2rLgpme",
        "colab_type": "text"
      },
      "source": [
        "#### Pointwise Feed forward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mNE0fJvefaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgfEMiove1Zn",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qQc6o64eySC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    # normalize data per feature instead of batch\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "    # Multi-head attention layer\n",
        "    attn_output, _ = self.mha(x, x, x, mask) \n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    # add residual connection to avoid vanishing gradient problem\n",
        "    out1 = self.layernorm1(x + attn_output)\n",
        "    \n",
        "    # Feedforward layer\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    # add residual connection to avoid vanishing gradient problem\n",
        "    out2 = self.layernorm2(out1 + ffn_output)\n",
        "    return out2"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqLdtAAPorvt",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WsCLoYEfC1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    # Create encoder layers (count: num_layers)\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6bSvwJ1e_g8",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQgt7OcdeyKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    # Masked multihead attention layer (padding + look-ahead)\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    # again add residual connection\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    # Masked multihead attention layer (only padding)\n",
        "    # with input from encoder as Key and Value, and input from previous layer as Query\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    # again add residual connection\n",
        "    out2 = self.layernorm2(attn2 + out1)\n",
        "    \n",
        "    # Feedforward layer\n",
        "    ffn_output = self.ffn(out2)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    # again add residual connection\n",
        "    out3 = self.layernorm3(ffn_output + out2)\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxvJ4M-pouYH",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8M_UsFHfCtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "     \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    # Create decoder layers (count: num_layers)\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    \n",
        "    x += self.pos_encoding[:,:seq_len,:]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      # store attenion weights, they can be used to visualize while translating\n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    return x, attention_weights"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wlWv0HSptaN",
        "colab_type": "text"
      },
      "source": [
        "#### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3ldNVklfaMY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    # Pass the input to the encoder\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "    \n",
        "    # Pass the encoder output to the decoder\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    # Pass the decoder output to the last linear layer\n",
        "    final_output = self.final_layer(dec_output)\n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLT6USM-htw3",
        "colab_type": "text"
      },
      "source": [
        "#### Optimizer and Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "588zG6wOfaGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfEAPe3FfZ-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "# Adam optimizer with a custom learning rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ree_sJLFfZfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il10DQXPefP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  # Apply a mask to paddings (0)\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzVTb8RmefDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6QXJV9rh8nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnNv_E9Wh8jl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Decoder padding mask\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Look ahead mask (for hiding the rest of the sequence in the 1st decoder attention layer)\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n7etu6ariHqK"
      },
      "source": [
        "### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UkZOgiXPiHqM",
        "colab": {}
      },
      "source": [
        "!rm -rf 'data/ADL Project'"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "72OsBxu6iHqO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2f7c7091-e9df-4443-9a39-b25f2a9bfffc"
      },
      "source": [
        "checkpoint_dir = os.path.join(drive_root, \"ADL Project/checkpoints\")\n",
        "checkpoint_dir = os.path.join(checkpoint_dir, \"training_checkpoints/akshata_transfomer\")\n",
        "\n",
        "print(\"Checkpoints directory is\", checkpoint_dir)\n",
        "if os.path.exists(checkpoint_dir):\n",
        "  print(\"Checkpoints folder already exists\")\n",
        "else:\n",
        "  print(\"Creating a checkpoints directory\")\n",
        "  os.makedirs(checkpoint_dir)\n",
        "\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoints directory is data/ADL Project/checkpoints/training_checkpoints/akshata_transfomer\n",
            "Creating a checkpoints directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kbxp9qB_iHqS",
        "colab": {}
      },
      "source": [
        "latest = ckpt_manager.latest_checkpoint\n",
        "latest"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ekNEXzDJiHqW",
        "colab": {}
      },
      "source": [
        "if latest:\n",
        "  epoch_num = int(latest.split('/')[-1].split('-')[-1])\n",
        "  checkpoint.restore(latest)\n",
        "  print ('Latest checkpoint restored!!')\n",
        "else:\n",
        "  epoch_num = 0"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "leaHQMBaiHqX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79eaed90-9bd5-48f5-b255-9728d2070234"
      },
      "source": [
        "epoch_num"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj0D4HuIatpn",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiVDtj2FoNZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 40"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIQBWfa03ujc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_results(inp_sentence):\n",
        "  start_token = [len(inp_lang_tokenizer.word_index)+1]\n",
        "  end_token = [len(inp_lang_tokenizer.word_index)+2]\n",
        "  \n",
        "  # inp sentence is the word problem, hence adding the start and end token\n",
        "  inp_sentence = start_token + list(inp_sentence.numpy()[0]) + end_token\n",
        "  \n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  \n",
        "  decoder_input = [old_len+1]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == old_len+2:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTs5Q4Qf2qNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up a dataset\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
        "dataset_evalue = dataset_val.batch(1, drop_remainder=False)\n",
        "dataset_val = dataset_val.batch(64, drop_remainder=False)   # 當 dataset 少於 batchsize 的情形下刪除最後一批資料"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6bhdYH24Zxw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efe7223a-ee4d-4af4-e5b9-1caa76496c03"
      },
      "source": [
        "BUFFER_SIZE"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wApee0Bd5aH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from multiprocessing import Pool\n",
        "\n",
        "def PredictIdx2Word(tensor_idxs, require_idx=False):\n",
        "  pre_sentence = []\n",
        "  tensor_idxs = tf.squeeze(tensor_idxs, axis=0)\n",
        "  for (idx, i) in enumerate(tensor_idxs.numpy()):\n",
        "    if i in [0, old_len+2] :    # end \n",
        "      break  \n",
        "    if (i < len(targ_lang_tokenizer.word_index) and i not in [0,old_len+1,old_len+2]):\n",
        "      pre_sentence.append(targ_lang_tokenizer.index_word[i])\n",
        "  if require_idx:\n",
        "    return pre_sentence, idx\n",
        "  else:\n",
        "    return pre_sentence\n",
        "\n",
        "def TarIdx2Word(tensor_idxs):\n",
        "  target_sentence = ''\n",
        "  tensor_idxs = tf.squeeze(tensor_idxs, axis=0)\n",
        "  for i in tensor_idxs.numpy():\n",
        "    if i not in [0,old_len+1,old_len+2]:\n",
        "      target_sentence += (targ_lang_tokenizer.index_word[i] + ' ')\n",
        "  target_sentence = target_sentence.split(' ')[:-1]\n",
        "  return target_sentence\n",
        "\n",
        "def InpIdx2Sec(tensor_idxs, require_idx=False):\n",
        "  inp_sentence = ''\n",
        "  tensor_idxs = tf.squeeze(tensor_idxs, axis=0)\n",
        "  for (idx, i) in enumerate(tensor_idxs.numpy()):\n",
        "    if i in [0, inp_len+2]:\n",
        "      break\n",
        "    if i not in [0,inp_len+1,inp_len+2]:\n",
        "      inp_sentence += (inp_lang_tokenizer.index_word[i] + ' ')\n",
        "  if require_idx:\n",
        "    return inp_sentence, idx\n",
        "  else:\n",
        "    return inp_sentence"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e8QVhZwxpKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calAnc(y_true, y_pred):\n",
        "\n",
        "  for i in range(len(y_true)):\n",
        "    for j in range(len(y_true[i])):\n",
        "      y_true[i][j] = ord(y_true[i][j])\n",
        "  for i in range(len(y_pred)):\n",
        "    for j in range(len(y_pred[i])):\n",
        "      y_pred[i][j] = ord(y_pred[i][j])\n",
        "\n",
        "  y_true = tf.keras.preprocessing.sequence.pad_sequences(y_true, padding='post', value=3)\n",
        "  y_pred = tf.keras.preprocessing.sequence.pad_sequences(y_pred, padding='post', value=3)\n",
        "  # print(y_true[0])\n",
        "  # print(y_pred[0])\n",
        "  if y_true.shape[1] < y_pred.shape[1]:\n",
        "    y_pred = y_pred[:, :y_true.shape[1]]\n",
        "  else:\n",
        "    y_true = y_true[:, :y_pred.shape[1]]\n",
        "  \n",
        "  table = np.equal(y_true, y_pred)\n",
        "  # print(table[0])\n",
        "  table = np.multiply.reduce(table, axis=-1)\n",
        "  # print(np.count_nonzero(table>0), len(table))\n",
        "\n",
        "  return np.count_nonzero(table>0), len(table)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u17Cq4nYY19U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maskValidate(v_data, calLoss=False):\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "  for (batch, (inp, tar)) in enumerate(v_data):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                False, \n",
        "                                enc_padding_mask, \n",
        "                                combined_mask, \n",
        "                                dec_padding_mask)\n",
        "    if calLoss:\n",
        "      loss = loss_function(tar_real, predictions)\n",
        "      val_loss(loss)\n",
        "\n",
        "    predicted_max = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    list_predicted_max = tf.split(predicted_max, num_or_size_splits=len(predicted_max), axis=0)\n",
        "    list_target = tf.split(tar, num_or_size_splits=len(tar), axis=0)\n",
        "\n",
        "    with Pool(processes=8) as pool:\n",
        "      list_tar_sentence = pool.map(TarIdx2Word, list_target)\n",
        "      list_predicted_sentence = pool.map(PredictIdx2Word, list_predicted_max)\n",
        "    y_true.append(list_tar_sentence)\n",
        "    y_pred.append(list_predicted_sentence)\n",
        "  y_true = [val for sublist in y_true for val in sublist]\n",
        "  y_pred = [val for sublist in y_pred for val in sublist]\n",
        "  return y_true, y_pred"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOcltIu0h8cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  # train_accuracy(tar_real, predictions)\n",
        "\n",
        "def train_process(dataset, epoch=-1):\n",
        "  # 訓練模型 inp -> question, tar -> equation\n",
        "  dataset.shuffle(BUFFER_SIZE)\n",
        "  for (batch, (inp, tar)) in enumerate(dataset):\n",
        "    train_step(inp, tar)\n",
        "    \n",
        "    print ('\\r', 'Train [{:3d}] Batch [{:4d}] loss {:.4f} perplexity {:.4f}'.format(\n",
        "        epoch + 1, batch, train_loss.result(), np.exp(train_loss.result())), end='')\n",
        "\n",
        "def val_process(dataset, dataset_val, epoch=-1, evalue=False):\n",
        "  \n",
        "  train_y_true, train_y_pred = maskValidate(dataset)\n",
        "  val_y_true, val_y_pred = maskValidate(dataset_val, calLoss=True)\n",
        "\n",
        "  if (evalue == True):\n",
        "    train_anc, train_total = calAnc(copy.deepcopy(train_y_true), copy.deepcopy(train_y_pred))\n",
        "    val_anc, val_total = calAnc(copy.deepcopy(val_y_true), copy.deepcopy(val_y_pred))\n",
        "    if (epoch == -1):\n",
        "      return val_y_true, val_y_pred, val_anc, val_total\n",
        "    else:\n",
        "      print('\\nEpoch [{:3d}]  train_loss {:.4f}, train_bleu {:.4f}, train_acc {:.4f}, val_loss {:.4f}, val_bleu {:.4f}, val_acc {:.4f}'.format(\n",
        "                                                                epoch + 1,\n",
        "                                                                float(train_loss.result()),\n",
        "                                                                corpus_bleu(train_y_true, train_y_pred, smoothing_function=smoothie.method4),\n",
        "                                                                float(train_anc)/train_total,\n",
        "                                                                float(val_loss.result()),\n",
        "                                                                corpus_bleu(val_y_true, val_y_pred, smoothing_function=smoothie.method4),\n",
        "                                                                float(val_anc)/val_total,\n",
        "                                                                ))\n",
        "  elif not evalue:\n",
        "    print('\\nEpoch [{:3d}]             train_loss {:.4f}, val_loss {:.4f}'.format(epoch + 1, float(train_loss.result()), float(val_loss.result())))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEqQmYLZh8R3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "5c717b65-fca4-4905-90f7-ab0fc50d4270"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(epoch_num, EPOCHS):\n",
        "  start = time.time()\n",
        "  train_loss.reset_states()\n",
        "  val_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  # 訓練模型\n",
        "  train_process(dataset, epoch=epoch)\n",
        "  # 檢驗模型\n",
        "  if (epoch+1) % 1 == 0:\n",
        "    val_process(dataset, dataset_val, epoch=epoch, evalue=True)\n",
        "        \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('\\nSaving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Train [  1] Batch [ 565] loss 0.1996 perplexity 1.2209\n",
            "Epoch [  1]  train_loss 0.1996, train_bleu 0.2878, train_acc 0.2439, val_loss 0.0699, val_bleu 0.2727, val_acc 0.2332\n",
            "\n",
            "Saving checkpoint for epoch 1 at data/ADL Project/checkpoints/training_checkpoints/akshata_transfomer/ckpt-1\n",
            "Time taken for 1 epoch: 385.0122504234314 secs\n",
            "\n",
            " Train [  2] Batch [ 565] loss 0.0531 perplexity 1.0545\n",
            "Epoch [  2]  train_loss 0.0531, train_bleu 0.3146, train_acc 0.5229, val_loss 0.0406, val_bleu 0.3057, val_acc 0.5000\n",
            "\n",
            "Saving checkpoint for epoch 2 at data/ADL Project/checkpoints/training_checkpoints/akshata_transfomer/ckpt-2\n",
            "Time taken for 1 epoch: 380.7758901119232 secs\n",
            "\n",
            " Train [  3] Batch [ 565] loss 0.0232 perplexity 1.0235\n",
            "Epoch [  3]  train_loss 0.0232, train_bleu 0.3114, train_acc 0.8921, val_loss 0.0169, val_bleu 0.3211, val_acc 0.9025\n",
            "\n",
            "Saving checkpoint for epoch 3 at data/ADL Project/checkpoints/training_checkpoints/akshata_transfomer/ckpt-3\n",
            "Time taken for 1 epoch: 379.5892560482025 secs\n",
            "\n",
            " Train [  4] Batch [ 565] loss 0.0166 perplexity 1.0167"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpi8JaiblDOh",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orKyYXqoyvkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For single sentence\n",
        "def val_translate(inp, tar, plot=''):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "  # predictions.shape == (1, seq_len, vocab_size)\n",
        "  predictions, attention_weights = transformer(inp, tar_inp, \n",
        "                              False, \n",
        "                              enc_padding_mask, \n",
        "                              combined_mask, \n",
        "                              dec_padding_mask)\n",
        "  predicted_max = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "  input_sentence, input_idx = InpIdx2Sec(inp ,require_idx=True)\n",
        "  tar_sentence = TarIdx2Word(tar)\n",
        "  predicted_sentence, idx = PredictIdx2Word(predicted_max, require_idx=True)\n",
        "  print('Input: {}'.format(input_sentence), '  input_idx:', input_idx)\n",
        "  print('Target translation: {}'.format(' '.join(tar_sentence)))\n",
        "  print('Predicted translation: {}'.format(' '.join(predicted_sentence)))\n",
        "  if plot:\n",
        "    plot_attention_weights(attention_weights, input_sentence, tf.squeeze(predicted_max, axis=0), plot, idx, input_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op5A3ikpil_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  start_token = [len(inp_lang_tokenizer.word_index)+1]\n",
        "  end_token = [len(inp_lang_tokenizer.word_index)+2]\n",
        "  \n",
        "  # inp sentence is the word problem, hence adding the start and end token\n",
        "  inp_sentence = start_token + [inp_lang_tokenizer.word_index[i] for i in preprocess_input(inp_sentence).split(' ')]+end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  # start with equation's start token\n",
        "  decoder_input = [old_len+1]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :] \n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == old_len+2:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "  return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myLyxXYpiltz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer, idx=-1, input_idx=-1):\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "  sentence = preprocess_input(sentence)\n",
        "  attention = tf.squeeze(attention[layer], axis=0)\n",
        "  if idx!=-1:\n",
        "    print('idx: {}, input_idx: {}'.format(idx, input_idx), '  attention.shape: ', attention[0][:idx, :input_idx+1].shape)\n",
        "  \n",
        "  for head in range(attention.shape[0]):\n",
        "    ax = fig.add_subplot(2, 4, head+1)\n",
        "    \n",
        "    # plot the attention weights\n",
        "    if input_idx != -1:\n",
        "      ax.matshow(attention[head][:idx, :input_idx+1], cmap='viridis')\n",
        "    else:\n",
        "      ax.matshow(attention[head][:idx, :], cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 10}\n",
        "    \n",
        "    ax.set_xticks(range(len(sentence.split(' '))+2))\n",
        "\n",
        "    align_seq = []\n",
        "    for i in list(result.numpy()):\n",
        "      if i in [0, old_len+2]:\n",
        "        break\n",
        "      if i < len(targ_lang_tokenizer.word_index) and i not in [0,old_len+1,old_len+2]:\n",
        "        align_seq.append(targ_lang_tokenizer.index_word[i])\n",
        "\n",
        "    ax.set_yticks(range(len(align_seq)+3))\n",
        "    \n",
        "    \n",
        "    ax.set_ylim(len(align_seq), -0.5)\n",
        "\n",
        "    ax.set_xticklabels(\n",
        "        ['<start>']+sentence.split(' ')+['<end>'], \n",
        "        fontdict=fontdict, rotation=90)\n",
        "    \n",
        "    ax.set_yticklabels(align_seq, fontdict=fontdict)\n",
        "    \n",
        "    ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joZ3POuph8NH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence, plot=''):\n",
        "  result, attention_weights = evaluate(sentence)\n",
        "  # print('result',list(result.numpy()))\n",
        "\n",
        "  # use the result tokens to convert prediction into a list of characters\n",
        "  # (not inclusing padding, start and end tokens)\n",
        "  predicted_sentence = TarIdx2Word(tf.expand_dims(result, axis=0))  \n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted translation: {}'.format(' '.join(predicted_sentence)))\n",
        "  \n",
        "  if plot:\n",
        "    plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spiU0QDI2egM",
        "colab_type": "text"
      },
      "source": [
        "### Get Accuracy and Corpus BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVyDC8zyo0uM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true, y_pred, acc_cnt, total = val_process(dataset_val, evalue=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ1ZmlHovmSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_true = []\n",
        "# y_pred = []\n",
        "# acc_cnt = 0\n",
        "\n",
        "# a = 0\n",
        "# for (inp_val_batch, target_val_batch) in iter(dataset_evalue):\n",
        "#   a += 1\n",
        "#   if a % 50 == 0:\n",
        "#     print(a)\n",
        "#     print(\"Accuracy count: \",acc_cnt)\n",
        "#     print('------------------')\n",
        "#   target_sentence = ''\n",
        "#   for i in target_val_batch.numpy()[0]:\n",
        "#     if i not in [0,old_len+1,old_len+2]:\n",
        "#       target_sentence += (targ_lang_tokenizer.index_word[i] + ' ')\n",
        "  \n",
        "#   y_true.append([target_sentence.split(' ')[:-1]])\n",
        "  \n",
        "#   result, _ = evaluate_results(inp_val_batch)\n",
        "#   predicted_sentence = [targ_lang_tokenizer.index_word[i] for i in list(result.numpy()) if (i < len(targ_lang_tokenizer.word_index) and i not in [0,old_len+1,old_len+2])] \n",
        "#   y_pred.append(predicted_sentence)\n",
        "  \n",
        "#   if target_sentence.split(' ')[:-1] == predicted_sentence:\n",
        "#     acc_cnt += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daGaIvQW7riI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Corpus BLEU score of the model: ', corpus_bleu(y_true, y_pred, smoothing_function=smoothie.method4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcswXaKS5ssh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy of the model: ', acc_cnt/total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V_Mk0_s-Jcx",
        "colab_type": "text"
      },
      "source": [
        "### Attention Weights on Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUVktoQuZsRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp, tar = random.choice(list(dataset))\n",
        "idx = random.choice(range(40))\n",
        "inp = tf.expand_dims(inp[idx], axis=0)\n",
        "tar = tf.expand_dims(tar[idx], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq-o342pZtCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_str = InpIdx2Sec(inp)\n",
        "print('Target translation: ', ' '.join(TarIdx2Word(tar)))\n",
        "translate(check_str, plot='decoder_layer4_block2')                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VB3-w1QZxLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_translate(inp, tar, 'decoder_layer4_block2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SFJifWj7AtQ",
        "colab_type": "text"
      },
      "source": [
        "#### Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GEmx2HVO6_cC",
        "colab": {}
      },
      "source": [
        "check_str = ' '.join([inp_lang_tokenizer.index_word[i] for i in input_tensor_val[32] if i not in [0,\n",
        "                                                                                                  len(inp_lang_tokenizer.word_index)+1,\n",
        "                                                                                                  len(inp_lang_tokenizer.word_index)+2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A_fyWhFZ6_cG",
        "colab": {}
      },
      "source": [
        "check_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-fPqedk46_cM",
        "colab": {}
      },
      "source": [
        "translate(check_str,\n",
        "          plot='decoder_layer4_block2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8Pm4N7Xo6_cN",
        "colab": {}
      },
      "source": [
        "check_str = ' '.join([inp_lang_tokenizer.index_word[i] for i in input_tensor_val[0] if i not in [0,\n",
        "                                                                                                  len(inp_lang_tokenizer.word_index)+1,\n",
        "                                                                                                  len(inp_lang_tokenizer.word_index)+2]])\n",
        "check_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M4zBzmcq6_cP",
        "colab": {}
      },
      "source": [
        "translate(check_str,\n",
        "          plot='decoder_layer4_block2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EUVJbPnJ6_cV",
        "colab": {}
      },
      "source": [
        "check_str = ' '.join([inp_lang_tokenizer.index_word[i] for i in input_tensor_val[234] if i not in [0,\n",
        "                                                                                                  len(inp_lang_tokenizer.word_index)+1,\n",
        "                                                                                                  len(inp_lang_tokenizer.word_index)+2]])\n",
        "check_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RFbRPr916_cY",
        "colab": {}
      },
      "source": [
        "translate(check_str,\n",
        "          plot='decoder_layer4_block2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xQOswvCM6_cZ",
        "colab": {}
      },
      "source": [
        "translate(\"Jerry had 135 pens. John took 19 from him. How many pens Jerry have left?\",\n",
        "          plot='decoder_layer4_block2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ERVfc2FKJ8",
        "colab_type": "text"
      },
      "source": [
        "### Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPktUoq4FKFw",
        "colab_type": "text"
      },
      "source": [
        "We see that the scores for the transformer model are much higher than the Bidirectional Seq2Seq model, and that the attention plots are also better (all the layers and heads) than the previous model. This indicates that this model performs better on our data than the Seq2Seq model. BUt we can see that it has still not been able to learn which words to give proper attention to while deciding the operator to use. \n",
        "Moreover, the issue of the dataset not being diverse enough still remains, and this model does not perform very well on questions that do not match the rough format of the questions in the data. \n",
        "\n",
        "Sources:\n",
        "1. https://web.stanford.edu/class/cs224n/reports/custom/15843468.pdf\n",
        "2. https://www.tensorflow.org/tutorials/text/transformer#top_of_page"
      ]
    }
  ]
}